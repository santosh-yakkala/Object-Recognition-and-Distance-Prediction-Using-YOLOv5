{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMvFB+oSZjrywVhfZNSWq4l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-sLdQmvYlhoJ","executionInfo":{"status":"ok","timestamp":1658040825251,"user_tz":-330,"elapsed":122587,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}},"outputId":"5c8f1b07-2b78-441e-9858-f8224c70e981"},"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 ðŸš€ v6.1-303-g2430578 Python-3.7.13 torch-1.11.0+cu102 CPU\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete âœ… (2 CPUs, 12.7 GB RAM, 40.9/107.7 GB disk)\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","display = utils.notebook_init()  # checks"]},{"cell_type":"code","source":["# to import the dataset from google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"ChuZuLl5FxWy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658040854406,"user_tz":-330,"elapsed":21721,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}},"outputId":"8900935f-4f97-429a-fd5c-b6b5dba0c5ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!unzip -q ../gdrive/MyDrive/train_data.zip -d ../"],"metadata":{"id":"t2e0kZIFF3Rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q ../train_data.zip -d ../"],"metadata":{"id":"1a1B9Wu_l-CE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train YOLOv5s on custom_data for 50 epochs\n","!python train.py --img 640 --batch 8 --epochs 50 --data ../custom_data.yaml --weights ../gdrive/MyDrive/yolov5s.pt --cache"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rTd08EENqUq6","outputId":"2bd1347e-9a7b-4202-94cc-9a0889348856","executionInfo":{"status":"ok","timestamp":1658041081397,"user_tz":-330,"elapsed":192882,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=../gdrive/MyDrive/yolov5s.pt, cfg=, data=../custom_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ðŸš€ v6.1-303-g2430578 Python-3.7.13 torch-1.11.0+cu102 CPU\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 101MB/s]\n","Overriding model.yaml nc=80 with nc=16\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     56637  models.yolo.Detect                      [16, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 270 layers, 7062781 parameters, 7062781 gradients, 16.1 GFLOPs\n","\n","Transferred 343/349 items from ../gdrive/MyDrive/yolov5s.pt\n","Scaled weight_decay = 0.0005\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/../train_data/labels/train' images and labels...5515 found, 0 missing, 0 empty, 0 corrupt: 100% 5515/5515 [00:03<00:00, 1798.29it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/../train_data/labels/train.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.8GB ram): 100% 5515/5515 [00:28<00:00, 194.91it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/../train_data/labels/val' images and labels...1055 found, 0 missing, 1 empty, 0 corrupt: 100% 1055/1055 [00:00<00:00, 1214.90it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/../train_data/labels/val.cache\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (1.3GB ram): 100% 1055/1055 [00:05<00:00, 189.08it/s]\n","Plotting labels to runs/train/exp/labels.jpg... \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.37 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 50 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls    labels  img_size\n","      0/49        0G    0.1096   0.03793   0.07401        24       640:   1% 10/690 [01:52<2:07:13, 11.23s/it]\n","Traceback (most recent call last):\n","  File \"train.py\", line 667, in <module>\n","    main(opt)\n","  File \"train.py\", line 562, in main\n","    train(opt.hyp, opt, device, callbacks)\n","  File \"train.py\", line 352, in train\n","    pred = model(imgs)  # forward\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/yolov5/models/yolo.py\", line 135, in forward\n","    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n","  File \"/content/yolov5/models/yolo.py\", line 158, in _forward_once\n","    x = m(x)  # run\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/yolov5/models/common.py\", line 47, in forward\n","    return self.act(self.bn(self.conv(x)))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 447, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 444, in _conv_forward\n","    self.padding, self.dilation, self.groups)\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["# storing the results\n","!zip -r ../results.zip ./runs/train/exp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5I_6yB5nMbcC","executionInfo":{"status":"ok","timestamp":1657983301899,"user_tz":-330,"elapsed":2391,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}},"outputId":"f6a5a9ab-3472-4683-9646-07d9bbee3a30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: runs/train/exp/ (stored 0%)\n","  adding: runs/train/exp/train_batch0.jpg (deflated 11%)\n","  adding: runs/train/exp/train_batch1.jpg (deflated 8%)\n","  adding: runs/train/exp/R_curve.png (deflated 9%)\n","  adding: runs/train/exp/val_batch0_pred.jpg (deflated 10%)\n","  adding: runs/train/exp/val_batch0_labels.jpg (deflated 12%)\n","  adding: runs/train/exp/hyp.yaml (deflated 45%)\n","  adding: runs/train/exp/confusion_matrix.png (deflated 19%)\n","  adding: runs/train/exp/labels_correlogram.jpg (deflated 18%)\n","  adding: runs/train/exp/opt.yaml (deflated 42%)\n","  adding: runs/train/exp/P_curve.png (deflated 11%)\n","  adding: runs/train/exp/val_batch1_pred.jpg (deflated 12%)\n","  adding: runs/train/exp/F1_curve.png (deflated 9%)\n","  adding: runs/train/exp/train_batch2.jpg (deflated 9%)\n","  adding: runs/train/exp/val_batch2_pred.jpg (deflated 13%)\n","  adding: runs/train/exp/results.png (deflated 8%)\n","  adding: runs/train/exp/events.out.tfevents.1657976071.f719faa2b0e7.381.0 (deflated 29%)\n","  adding: runs/train/exp/val_batch2_labels.jpg (deflated 13%)\n","  adding: runs/train/exp/labels.jpg (deflated 12%)\n","  adding: runs/train/exp/weights/ (stored 0%)\n","  adding: runs/train/exp/weights/last.pt (deflated 9%)\n","  adding: runs/train/exp/weights/best.pt (deflated 9%)\n","  adding: runs/train/exp/val_batch1_labels.jpg (deflated 14%)\n","  adding: runs/train/exp/PR_curve.png (deflated 15%)\n","  adding: runs/train/exp/results.csv (deflated 83%)\n"]}]},{"cell_type":"code","source":["# results of the training\n","!python val.py --data ../custom_data.yaml --weights ./runs/train/exp2/weights/best.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ouVcusKpoGyu","executionInfo":{"status":"ok","timestamp":1655528319713,"user_tz":-330,"elapsed":20044,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}},"outputId":"9c955477-c1db-46df-a5c5-aba9c1c1f33b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=../custom_data.yaml, weights=['./runs/train/exp2/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ðŸš€ v6.1-256-gd605138 Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","\n","Fusing layers... \n","Model summary: 213 layers, 7053277 parameters, 0 gradients\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/../train_data/labels/val.cache' images and labels... 817 found, 0 missing, 2 empty, 0 corrupt: 100% 817/817 [00:00<?, ?it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 26/26 [00:10<00:00,  2.42it/s]\n","                 all        817        966      0.999        0.5      0.746      0.611\n","              bottle        817        523          1          1      0.995      0.812\n","        mobile phone        817          1          1          0      0.995      0.796\n","               socks        817        441      0.997          1      0.995      0.837\n","                sofa        817          1          1          0          0          0\n","Speed: 0.2ms pre-process, 3.1ms inference, 1.1ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"]}]},{"cell_type":"code","source":["# detecting the objects in a given type of source\n","!python detect.py --weights ../final-weights.pt --conf 0.4 --source 0  # webcam\n","                                                                    img.jpg  # image\n","                                                                    vid.mp4  # video\n","                                                                    path/  # directory\n","                                                                    'path/*.jpg'  # glob\n","                                                                    'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                                                                    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-QRVntNFUQJE","executionInfo":{"status":"ok","timestamp":1655530919614,"user_tz":-330,"elapsed":14881,"user":{"displayName":"Santosh Yakkala","userId":"12621136798477414273"}},"outputId":"6691606e-f4df-4706-a4d7-99d46ae6c42d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['../final-weights.pt'], source=0, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n","YOLOv5 ðŸš€ v6.1-256-gd605138 Python-3.7.13 torch-1.11.0+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","\n","Fusing layers... \n","Model summary: 213 layers, 7053277 parameters, 0 gradients\n","WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\n","cv2.imshow() is disabled in Google Colab environments\n","Traceback (most recent call last):\n","  File \"detect.py\", line 252, in <module>\n","    main(opt)\n","  File \"detect.py\", line 247, in main\n","    run(**vars(opt))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"detect.py\", line 100, in run\n","    dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n","  File \"/content/yolov5/utils/dataloaders.py\", line 325, in __init__\n","    assert not is_colab(), '--source 0 webcam unsupported on Colab. Rerun command in a local environment.'\n","AssertionError: --source 0 webcam unsupported on Colab. Rerun command in a local environment.\n"]}]}]}